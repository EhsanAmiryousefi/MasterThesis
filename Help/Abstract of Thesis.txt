OpenMP Support for Data Dependence Profiler

Abstract

The application of the parallel systems is growing very fast as the advancement in the processors production has slowed. In other words, due to the existing limitations on single-core CPUs performance improvement, this improvement should be applied by utilizing multicore systems and consequently, to utilize the Multicore Systems we need to apply parallelism in our Software implementations. 

In the presence of the parallelism tools like OpenMP the application of Parallelism in a program might seem to be easy. The na•ve approach would be finding all of the potential parallelism in the program based on the static analysis and apply the parallelism on it. However, using parallelism in an efficient manner needs a concrete analysis of the existing system in terms of the threads behaviour in run-time and the communications between cooperating threads which requires dynamic analysis. In other words, if the process of applying the parallelism in a system is not done efficiently, the overhead of the parallelism may lead to a result which is not expected or favourable. For instance, if two highly cooperating threads are not placed in two physically adjacent cores (or even same cores), this can lead to a high overhead caused by communication time. Hence, the challenge is not only to parallelize the programs to utilize the multi-core systems but also to apply the parallelism in the most optimized manner to reduce the side-effects of the parallelism.
In order to cope with this we need to analyse the behaviour of the parallelized program first, and after profiling its behaviour we need to extract a concrete communication pattern between threads. Using the extracted communication pattern, we may specify a logical mapping of the threads location on the physical topology of the cores based on the communication between the threads. It goes without saying, that the process of Analysing hundreds of thousands lines of codes manually can be overwhelming. on the other hand, if this data dependence analysis is based on a static analysis it can cause a not completely efficient parallelism, which means, not all of the potential parallelism is utilized. 

The purpose of this thesis is to extract these communications patterns between the interacting threads inside a program and distribute each of these threads on the physical cores based on the extracted pattern, which is, the highly communicating threads should be placed on the cores as near as possible in order to reduce the overhead of the latency produced by the communication. Furthermore, based on this pattern we may extract the series of time that each of the communicating threads have been interacting with each other, and analyse these series whether to keep the respected core for a certain amount of time on or shut it down for the sake of the energy consumption. 

There are different phases during this thesis from implementation to analysis and evaluation. The first phase would be to implement an instrumenting pass in order to profile the source code of the parallelized program using OpenMP to find different parts of it which are having memory accesses. We will instrument the code by injecting some meta-data like Thread ID, Access Time, type of Access etc., Then, based on this memory accesses pattern, we will find the communicating threads based on memory references. Finally, based on the analysis of the implemented instrumentation we will analyse the communication between threads and the distribution model of the threads should be specified based on this communication pattern. The communication pattern refers to the way the cooperating threads are having interactions with each other, the time they have this interaction, the frequency of this interaction and the length of time of this interaction etc., 

All in all, we will analyse the instrumented code output with regard to its correctness compared to the original code, to ensure that the injected codes are not affecting the program correctness.


Ehsan Amiryousefi
